aws_common: &aws_common
  aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
  aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
  aws_region_name: os.environ/AWS_REGION_NAME

openai_key: &openai_key
  api_key: os.environ/OPENAI_API_KEY

gemini_key: &gemini_key
  api_key: os.environ/GEMINI_API_KEY

model_list:
  - model_name: bedrock-nova-pro
    litellm_params:
      model: bedrock/amazon.nova-pro-v1:0
      <<: *aws_common

  - model_name: bedrock-claude-3-7
    litellm_params:
      model: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0
      <<: *aws_common

  - model_name: bedrock-claude-3-5-haiku
    litellm_params:
      model: bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0
      <<: *aws_common

  - model_name: gemini-2.0-flash-exp
    litellm_params:
      model: gemini/gemini-2.0-flash-exp
      <<: *gemini_key

  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      <<: *openai_key

litellm_settings:
  drop_params: True
  # if you want to start sending LLM Logs to Langfuse. Make sure to set `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` in your env
  # NOTE: since AI commands (you need to create one yourself, the built-in one can't change model nor prompt) won't preserve chat history
  # Use Langfuse for tracing can mitigate that (and additionally, you can see how Raycast form the system prompt, etc.)
  # https://langfuse.com/pricing
  success_callback: ["langfuse"]
  # success_callback: ["mlflow"]
  # failure_callback: ["mlflow"]

general_settings: 
  master_key: sk-1234
